# -*- coding: utf-8 -*-
"""Spam_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oCclAMu3i_4CyNH0a6s-i9LE8mlYTf3v

1)Reading Data & Visualization
"""

import pandas as pd
import nltk

df = pd.read_csv("/content/drive/MyDrive/spam.csv",encoding="latin-1")

df.head(5)

df.shape

# Drop unwanted columns
df.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace=True)

# Change the names of the columns #inplace is to replace th
df.rename(columns={'v1':'class','v2':'sms'},inplace=True)
df.sample(5)

# Groupby shows the total data and unique data that did not repeated in all categories
df.groupby('class').describe()

# drop duplicates by keeping the first value
df = df.drop_duplicates(keep='first')

df.groupby('class').describe()

# add Length column to the table that displys the lenght of the message
df["Length"] = df["sms"].apply(len)

df.head(2)

# display the message length in a histogram
df.hist(column='Length',by='class', bins=50)

"""2)Preprocessing"""

from nltk.stem.porter import PorterStemmer     # class for Stemming (Technique in NLP to reduce words to its root form)

nltk.download('stopwords')                    # Dataset to remove common words(eg- the, in, a, an)
from nltk.corpus import stopwords             # import stopwords in NLTK corpus. (Natural Language Toolkit)

nltk.download('punkt')                        # tokenizer model (To break text in to seperate words)
ps = PorterStemmer()

df.head(5)

import string

def clean_text(text):   #define a method to clean the dataset
  text = text.lower()   #set all letters to lowercase
  text = nltk.word_tokenize(text)   #Break text in to seperate words

  y = []    #Initialize an empty list
  for i in text:
    if i.isalnum():   #Make sure the each letter is contained alphanumeric values
      y.append(i)   #add that data to y list

  text = y[:]   #add all text in the iteration to the text list
  y.clear()   #clear the y list to make it ready for the next iteration

  for i in text:
    if i not in stopwords.words('english') and i not in string.punctuation:   #validate text data with the stopwords
      y.append(i)   #if not add those words to the list

  text = y[:]
  y.clear()

  for i in text:
    y.append(ps.stem(i))    #ps = PorterStemmer() find stems and stemed words are append to the list

  return " ".join(y)    #return all the stemmed words. Words seperated by spaces

# call clean_text method to message values and create new column
df['sms_cleaned'] = df['sms'].apply(clean_text)

df.head(5)

"""3)Feature Extraction"""

from sklearn.feature_extraction.text import TfidfVectorizer

tf_vec = TfidfVectorizer(max_features=3000)             # set the size of the vocabulary to 3000
X = tf_vec.fit_transform(df['sms_cleaned']).toarray()   # transform text to vector

X.shape

# Output is the category
Y = df['class'].values

"""4)Learning"""

from sklearn.model_selection import train_test_split

X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=2)   # split data as train and test

from sklearn.naive_bayes import MultinomialNB       # import the Naive_bayes ML model

model = MultinomialNB()
model.fit(X_train,Y_train)                         # Feed the train data in to the model

from sklearn.metrics import accuracy_score
y_pred = model.predict(X_test)                  # feed x_test data for the test prediction
print(accuracy_score(Y_test,y_pred))            # calculate the accuracy of the predicted data with the actual y_test data